{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, passages, questions, p_tokenizer, q_tokenizer):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.p_tokenizer = p_tokenizer\n",
    "        self.q_tokenizer = q_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.passages)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        passage = self.passages[index]\n",
    "        question = self.questions[index]\n",
    "        return passage, question\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        passages, questions = zip(*batch)\n",
    "        passage_inputs = self.p_tokenizer.batch_encode_plus(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        question_inputs = self.q_tokenizer.batch_encode_plus(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return passage_inputs, question_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model & tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "p_encoder = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "q_encoder = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "p_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "q_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer, scheduler, device, batch):\n",
    "    p_encoder.train()\n",
    "    q_encoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for passage_inputs, question_inputs in pbar:\n",
    "        passage_inputs = {k: v.to(device) for k, v in passage_inputs.items()}\n",
    "        question_inputs = {k: v.to(device) for k, v in question_inputs.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passage_embeddings = p_encoder(**passage_inputs).pooler_output\n",
    "        question_embeddings = q_encoder(**question_inputs).pooler_output\n",
    "\n",
    "        sim_scores = torch.matmul(question_embeddings, torch.transpose(passage_embeddings, 0, 1))\n",
    "\n",
    "        targets = torch.arange(0, batch).long().to(device)\n",
    "\n",
    "        sim_scores = torch.nn.functional.log_softmax(sim_scores, dim=1)\n",
    "        loss = torch.nn.functional.nll_loss(sim_scores, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix({\"Loss\" : loss.item()})\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "warmup_steps = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "traindata = pd.read_csv(\"train.csv\")\n",
    "question = pd.read_csv(\"question.csv\")\n",
    "collection = pd.read_csv(\"collection.csv\")\n",
    "evaldata = pd.read_csv(\"test.csv\")\n",
    "\n",
    "question.columns = ['0', 'x_id', 'content']\n",
    "collection.columns = ['0', 'y_id', 'document']\n",
    "\n",
    "traindata = traindata.merge(question, on=\"x_id\", how=\"left\")\n",
    "\n",
    "traindata.dropna(inplace = True)\n",
    "traindata.reset_index(inplace = True, drop=True)\n",
    "\n",
    "traindata = traindata.merge(collection, on=\"y_id\", how=\"left\")\n",
    "traindata.dropna(inplace=True)\n",
    "traindata.reset_index(inplace=True, drop=True)\n",
    "\n",
    "traindata = traindata[[\"content\", \"document\"]]\n",
    "traindata.columns = [\"question\", \"context\"]\n",
    "\n",
    "evaldata = evaldata.merge(question, on=\"x_id\", how=\"left\")\n",
    "\n",
    "evaldata.dropna(inplace = True)\n",
    "evaldata.reset_index(inplace = True, drop=True)\n",
    "\n",
    "evaldata = evaldata.merge(collection, on=\"y_id\", how=\"left\")\n",
    "evaldata.dropna(inplace=True)\n",
    "evaldata.reset_index(inplace=True, drop=True)\n",
    "\n",
    "evaldata = evaldata[[\"content\", \"document\"]]\n",
    "evaldata.columns = [\"question\", \"context\"]\n",
    "\n",
    "train_data = traindata[:50000]\n",
    "eval_data = evaldata\n",
    "\n",
    "train_passages = list(train_data[\"context\"])\n",
    "train_questions = list(train_data[\"question\"])\n",
    "train_dataset = DPRDataset(train_passages, train_questions, p_tokenizer, q_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "eval_passages = list(eval_data[\"context\"])\n",
    "eval_questions = list(eval_data[\"question\"])\n",
    "eval_dataset = DPRDataset(eval_passages, eval_questions, p_tokenizer, q_tokenizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters =[\n",
    "    {'params': [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 4.00 GiB total capacity; 3.02 GiB already allocated; 0 bytes free; 3.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m q_encoder\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train(train_dataloader, optimizer, scheduler, device, batch_size)\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m .4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, optimizer, scheduler, device, batch)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnll_loss(sim_scores, targets)\n\u001b[0;32m     22\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 23\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     24\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\rinap\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rinap\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rinap\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\rinap\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:160\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     amsgrad \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[0;32m    161\u001b[0m         group,\n\u001b[0;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m    163\u001b[0m         grads,\n\u001b[0;32m    164\u001b[0m         amsgrad,\n\u001b[0;32m    165\u001b[0m         exp_avgs,\n\u001b[0;32m    166\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    167\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         state_steps,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\rinap\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:118\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    114\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(\n\u001b[0;32m    115\u001b[0m     p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(\n\u001b[0;32m    119\u001b[0m     p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    122\u001b[0m     \u001b[39m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     state[\u001b[39m\"\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(\n\u001b[0;32m    124\u001b[0m         p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format\n\u001b[0;32m    125\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 4.00 GiB total capacity; 3.02 GiB already allocated; 0 bytes free; 3.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "p_encoder.to(device)\n",
    "q_encoder.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_dataloader, optimizer, scheduler, device, batch_size)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = pd.read_csv(\"collection.csv\")\n",
    "documents = list(passages['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "\n",
    "    # passage embeddings\n",
    "    p_embs = []\n",
    "    num_documents = len(documents)\n",
    "\n",
    "    for p in tqdm(documents, desc=\"Computing Passage Embeddings\", total=num_documents):\n",
    "        p = p_tokenizer(p, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        p_emb = p_encoder(**p).pooler_output.to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "    p_embs = torch.Tensor(p_embs).squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "\n",
    "    # passage embeddings\n",
    "    p_embs = []\n",
    "    num_documents = len(documents)\n",
    "\n",
    "    for p in tqdm(documents, desc=\"Computing Passage Embeddings\", total=num_documents):\n",
    "        p = p_tokenizer(p, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        p_emb = p_encoder(**p).pooler_output.to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "    p_embs = torch.Tensor(p_embs).squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    recall_1 = 0\n",
    "    recall_10 = 0\n",
    "    recall_20 = 0\n",
    "    recall_100 = 0\n",
    "\n",
    "    total_actual_positives = 0\n",
    "\n",
    "    q_encoder.eval()\n",
    "\n",
    "    for sample_idx in tqdm(range(len(eval_questions))):\n",
    "        query = eval_questions[sample_idx]\n",
    "\n",
    "        q_seqs_val = q_tokenizer([query], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        q_emb = q_encoder(**q_seqs_val).pooler_output.to(device)\n",
    "\n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "        correct_passage = eval_questions[sample_idx]\n",
    "\n",
    "        correct_idx = (documents.index(correct_passage) == rank).nonzero()\n",
    "\n",
    "        if correct_idx.numel() > 0:\n",
    "            correct_idx = correct_idx.item()\n",
    "\n",
    "            if correct_idx < 1:\n",
    "                recall_1 += 1\n",
    "            if correct_idx < 10:\n",
    "                recall_10 += 1\n",
    "            if correct_idx < 20:\n",
    "                recall_20 += 1\n",
    "            if correct_idx < 100:\n",
    "                recall_100 += 1\n",
    "        \n",
    "        total_actual_positives += 1\n",
    "    \n",
    "    recall_1 /= total_actual_positives\n",
    "    recall_10 /= total_actual_positives\n",
    "    recall_20 /= total_actual_positives\n",
    "    recall_100 /= total_actual_positives\n",
    "\n",
    "    print(\"Recall@1: \", recall_1)\n",
    "    print(\"Recall@10: \", recall_10)\n",
    "    print(\"Recall@20: \", recall_20)\n",
    "    print(\"Recall@100\", recall_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model save & load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.to(\"cpu\")\n",
    "q_encoder.to(\"cpu\")\n",
    "\n",
    "torch.save({\n",
    "    \"p_encoder_state_dict\": p_encoder.state_dict(),\n",
    "    \"q_encoder_state_dict\": q_encoder.state_dict(),\n",
    "}, \"encoder_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(\"encoder_new.pt\", map_location=\"cpu\")\n",
    "p_encoder.load_state_dict(model_state_dict[\"p_encoder_state_dict\"])\n",
    "q_encoder.load_state_dict(model_state_dict['q_encoder_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
